

The key technique for optimizing nearly any deep learning model is ittertively reducing the error by updating the parameters in the direction that incrementally lowers the loss function 

Some of those algorithms 

[[Minibatch Stochastic Gradient Descent]]